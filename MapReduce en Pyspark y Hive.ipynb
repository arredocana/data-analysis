{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M7 - Tarea 1\n",
    "\n",
    "Map & Reduce en Pyspark - Hive/Hue\n",
    "\n",
    "Para practicar los conceptos aprendidos os entregamos dos ficheros de datos:\n",
    "\n",
    "1. Contiene las provincias que componen cada comunidad autónoma. (Comunidades_y_provincias.csv)\n",
    "2. Contiene el número de contratos desglosados por provincia, municipio y sexo a lo largo de 2016 (Contratos_por_municipio.csv)\n",
    "\n",
    "Y la pregunta que nos gustaría que tenéis que respondenos es:\n",
    "\n",
    "**¿Qué comunidades autónomas han realizado más contratos a mujeres que a hombres durante todo el periodo?**\n",
    "\n",
    "El estudiante realizará la tarea utilizando dos formas de trabajar distintas que hemos explicado en este módulo, pero comprobará que el resultado final será idéntico.\n",
    "\n",
    "* Utilizando pyspark (Sin utilizar el módulo SparkSQL).\n",
    "* Utilizando Hive/Hue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "# Importamos PySpark e iniciamos el objeto SparkContext\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Importamos SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos datasets y convertimos en dataframe\n",
    "comunidades_df=sqlCtx.read.csv(\"Comunidades_y_provincias.csv\",sep=';',header=True)\n",
    "contratos_df=sqlCtx.read.csv('Contratos_por_municipio.csv',sep=';',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Comunidad_Autonoma: string (nullable = true)\n",
      " |-- Provincia: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- codigo_mes: string (nullable = true)\n",
      " |-- provincia: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- total_contratos: string (nullable = true)\n",
      " |-- contratos_hombres: string (nullable = true)\n",
      " |-- contratos_mujeres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos esquemas de los dataframes y el tipo de dato\n",
    "comunidades_df.printSchema()\n",
    "contratos_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- provincia: string (nullable = true)\n",
      " |-- contratos_mujeres: integer (nullable = true)\n",
      " |-- contratos_hombres: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos un nuevo dataframe 'contratos_clean' con las columnas necesarias para nuestro análisis\n",
    "# Además convertimos columnas 'contratos_hombres' y 'contratos_mujeres' a tipo entero (int) para poder sumarlos\n",
    "\n",
    "contratos_clean=contratos_df.select('provincia',contratos_df['contratos_mujeres'].cast('int'),\\\n",
    "                    contratos_df['contratos_hombres'].cast('int'))\n",
    "\n",
    "contratos_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------------+\n",
      "|  Comunidad_Autonoma|contratos_mujeres|contratos_hombres|\n",
      "+--------------------+-----------------+-----------------+\n",
      "|           Andalucia|          1047190|          1546405|\n",
      "|              Aragon|           142618|           182148|\n",
      "|Asturias, Princip...|            94136|            98137|\n",
      "|      Balears, Illes|           144575|           171866|\n",
      "|            Canarias|           201260|           225585|\n",
      "|           Cantabria|            67813|            67918|\n",
      "|Castilla - La Mancha|           175219|           276056|\n",
      "|     Castilla y Leon|           219930|           241998|\n",
      "|            Cataluna|           821496|           915859|\n",
      "|               Ceuta|             4084|             5005|\n",
      "|Comunitat Valenciana|           436580|           608068|\n",
      "|         Extremadura|           136266|           213457|\n",
      "|             Galicia|           250900|           278054|\n",
      "|Madrid, Comunidad de|           618013|           710875|\n",
      "|             Melilla|             5708|             5188|\n",
      "|   Murcia, Region de|           180291|           378124|\n",
      "|Navarra, Comunida...|           104844|            97782|\n",
      "|          Pais Vasco|           261310|           259770|\n",
      "|           Rioja, La|            36532|            43421|\n",
      "+--------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se podría realizar un paso intermedio donde agrupamos los contratos de hombres y mujeres por provincias\n",
    "#contratos_clean=contratos_clean.groupBy('provincia').sum()\n",
    "#contratos_clean.show()\n",
    "\n",
    "# Creamos nuevo dataframe 'tarea1_spark' resultado de la unión por provincias de contratos_clean y comunidades\n",
    "# Tipo de unión: inner join (por defecto) \n",
    "tarea1_spark=comunidades_df.join(contratos_clean,comunidades_df.Provincia==contratos_clean.provincia)\n",
    "\n",
    "# Agrupamos por comunidad autónoma y sumamos contratos, ordenamos alfabéticamente por comunidad autónoma\n",
    "tarea1_spark=tarea1_spark.groupBy('Comunidad_Autonoma').sum().orderBy('Comunidad_Autonoma')\n",
    "\n",
    "tarea1_spark=tarea1_spark.withColumnRenamed(\"sum(contratos_hombres)\", \"contratos_hombres\") \\\n",
    "    .withColumnRenamed(\"sum(contratos_mujeres)\", \"contratos_mujeres\")\n",
    "\n",
    "tarea1_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------------+\n",
      "|  Comunidad_Autonoma|contratos_mujeres|contratos_hombres|\n",
      "+--------------------+-----------------+-----------------+\n",
      "|          Pais Vasco|           261310|           259770|\n",
      "|Navarra, Comunida...|           104844|            97782|\n",
      "|             Melilla|             5708|             5188|\n",
      "+--------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos las comunidades donde hay más contratos de mujeres que de hombres\n",
    "tarea1_spark=tarea1_spark.filter(tarea1_spark['contratos_mujeres']>tarea1_spark['contratos_hombres'])\\\n",
    "    .orderBy('contratos_mujeres',ascending=0)\n",
    "\n",
    "# Guardamos en un archivo CSV\n",
    "#tarea1_spark.coalesce(1).write.csv(\"M7_tarea1_spark.csv\",sep=';',header=True)\n",
    "\n",
    "tarea1_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hive\n",
    "\n",
    "En la terminal de Hadoop escribiríamos el siguiente comando para conectarnos a Hive, ejecutar la consulta y guardar el resultado en un archivo CSV en nuestro directorio local de Hadoop:\n",
    "\n",
    "`hive -e 'set hive.cli.print.header=true; select a.comunidad comunidad_autonoma, sum(b.contratos_mujeres) num_contratos_mujeres, sum(b.contratos_hombres) num_contratos_hombres from uemc.comunidades a join uemc.contratos b on(a.provincia=b.provincia) group by a.comunidad having sum(b.contratos_mujeres) > sum(b.contratos_hombres) order by num_contratos_mujeres desc;' | sed 's/[\\t]/;/g' > /home/bigdata/data/M7_tarea1_hive.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark SQL y Hive\n",
    "\n",
    "Opcionalmente, se ha utilizado Databricks para realizar la misma consulta en Hive con Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de una tabla con comandos HIVE con Spark SQL\n",
    "# Creamos la BBDD donde vamos a colocar las tablas \n",
    "spark.sql('CREATE DATABASE IF NOT EXISTS uemc')\n",
    "spark.sql('SHOW DATABASES').show()\n",
    "\n",
    "# Creamos las tablas en HIVE\n",
    "spark.sql(\n",
    "  \"\"\"CREATE TABLE IF NOT EXISTS uemc.comunidades (comunidad STRING, provincia STRING)\n",
    "  ROW FORMAT DELIMITED\n",
    "  FIELDS TERMINATED BY ';'\n",
    "  LINES TERMINATED BY '\\n'\n",
    "  STORED AS TEXTFILE\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "  \"\"\"CREATE TABLE IF NOT EXISTS uemc.contratos (codigo_mes STRING, provincia STRING, municipio STRING,\n",
    "  total_contratos INT, contratos_hombres INT, contratos_mujeres INT)\n",
    "  ROW FORMAT DELIMITED\n",
    "  FIELDS TERMINATED BY ';'\n",
    "  LINES TERMINATED BY '\\n'\n",
    "  STORED AS TEXTFILE\"\"\"\n",
    ")\n",
    "\n",
    "# Comprobamos si las tablas se han creado\n",
    "spark.sql('USE uemc')\n",
    "spark.sql('SHOW TABLES').show()\n",
    "\n",
    "# Cargamos los datos en las tablas desde un fichero\n",
    "spark.sql(\"LOAD DATA INPATH '/FileStore/tables/Comunidades_y_provincias.csv' INTO TABLE uemc.comunidades\")\n",
    "spark.sql(\"LOAD DATA INPATH '/FileStore/tables/Contratos_por_municipio.csv' INTO TABLE uemc.contratos\")\n",
    "\n",
    "# Comprobamos los datos de las tablas\n",
    "spark.sql(\"SELECT * FROM uemc.comunidades LIMIT 5\").show()\n",
    "spark.sql(\"SELECT * FROM uemc.contratos LIMIT 5\").show()\n",
    "\n",
    "# Realizamos la consulta para obtener las comunidades que tienen más contratos de mujeres que de hombres\n",
    "spark.sql(\n",
    "  \"\"\"SELECT\n",
    "    a.comunidad comunidad_autonoma, \n",
    "    SUM(b.contratos_mujeres) num_contratos_mujeres, \n",
    "    SUM(b.contratos_hombres) num_contratos_hombres\n",
    "  FROM uemc.comunidades a JOIN uemc.contratos b ON (a.provincia=b.provincia) \n",
    "  GROUP BY a.comunidad \n",
    "  HAVING SUM(b.contratos_mujeres) > SUM(b.contratos_hombres)\n",
    "  ORDER BY num_contratos_mujeres DESC\"\"\"\n",
    ").show()\n",
    "\n",
    "spark.sql(\n",
    "  \"\"\"CREATE TABLE resultado AS \n",
    "  SELECT\n",
    "    a.comunidad comunidad_autonoma, \n",
    "    SUM(b.contratos_mujeres) num_contratos_mujeres, \n",
    "    SUM(b.contratos_hombres) num_contratos_hombres\n",
    "  FROM uemc.comunidades a JOIN uemc.contratos b ON (a.provincia=b.provincia) \n",
    "  GROUP BY a.comunidad \n",
    "  HAVING SUM(b.contratos_mujeres) > SUM(b.contratos_hombres)\n",
    "  ORDER BY num_contratos_mujeres DESC\"\"\"\n",
    ")\n",
    "\n",
    "# Creamos un DataFrame a partir de la tabla creada con el resultado de la consulta\n",
    "resultadoDF=spark.table('uemc.resultado')\n",
    "\n",
    "# Guardamos en un archivo CSV el DataFrame\n",
    "resultadoDF.coalesce(1).write.format('com.databricks.spark.csv').option('header','true').option('sep',';').save('dbfs:/FileStore/df/M7_tarea1_sparkhive.csv')\n",
    "\n",
    "# En el siguiente link descargamos en nuestro PC el archivo CSV \n",
    "#https://community.cloud.databricks.com/files/df/M7_tarea1_sparkhive.csv/part-00000-tid-2209616776404529798-85525cbf-2d99-40ff-b8c1-e3eca2a02957-2880-1-c000.csv?o=3781794380813136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografía\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast\n",
    "\n",
    "https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "https://nbviewer.jupyter.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb\n",
    "\n",
    "https://stackoverflow.com/questions/31385363/how-to-export-a-table-dataframe-in-pyspark-to-csv\n",
    "\n",
    "https://towardsdatascience.com/databricks-how-to-save-files-in-csv-on-your-local-computer-3d0c70e6a9ab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
